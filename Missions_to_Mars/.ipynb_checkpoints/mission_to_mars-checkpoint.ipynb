{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# downloading dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "executable_path = {'executable_path':'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the news_url and rigging up both requests and beautifulsoup\n",
    "news_url = 'https://mars.nasa.gov/news/'\n",
    "news_response = requests.get(news_url)\n",
    "soup = bs(news_response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Virginia Middle School Student Earns Honor of Naming NASA's Next Mars Rover\n",
      "\n",
      "\n",
      "\n",
      "NASA chose a seventh-grader from Virginia as winner of the agency's \"Name the Rover\" essay contest. Alexander Mather's entry for \"Perseverance\" was voted tops among 28,000 entries. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the title from the appropriate div and class\n",
    "news_title = soup.find('div', class_ = 'content_title').text\n",
    "print(news_title)\n",
    "# getting the first paragraph information from the appropriate div and class\n",
    "news_p = soup.find('div', class_ = 'rollover_description_inner').text\n",
    "print(news_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<splinter.element_list.ElementList at 0x169554c2f98>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the image site's url\n",
    "image_site_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "\n",
    "# visiting the site via splinter\n",
    "browser.visit(image_site_url)\n",
    "\n",
    "# clicking through the site via splinter to get to the large image\n",
    "browser.click_link_by_id('full_image')\n",
    "browser.click_link_by_partial_href('details')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mildl\\Anaconda2\\envs\\PythonData\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py:520: FutureWarning: browser.find_link_by_partial_href is deprecated. Use browser.links.find_by_partial_href instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "ElementDoesNotExist",
     "evalue": "no elements could be found with link by partial href \"largesize\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\PythonData\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ca2d87ab68eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# finally getting to the large image via splinter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick_link_by_partial_href\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'largesize'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# saving the url to f_image_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfeatured_image_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\PythonData\\lib\\site-packages\\splinter\\driver\\__init__.py\u001b[0m in \u001b[0;36mclick_link_by_partial_href\u001b[1;34m(self, partial_href)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[0mClicks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlink\u001b[0m \u001b[0mby\u001b[0m \u001b[0mlooking\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpartial\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \"\"\"\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_link_by_partial_href\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial_href\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclick_link_by_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\PythonData\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0melement_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0melement_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\PythonData\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     44\u001b[0m             raise ElementDoesNotExist(\n\u001b[0;32m     45\u001b[0m                 u'no elements could be found with {0} \"{1}\"'.format(\n\u001b[1;32m---> 46\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 )\n\u001b[0;32m     48\u001b[0m             )\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m: no elements could be found with link by partial href \"largesize\""
     ]
    }
   ],
   "source": [
    "# finally getting to the large image via splinter\n",
    "browser.click_link_by_partial_href('largesize')\n",
    "\n",
    "# saving the url to f_image_url\n",
    "featured_image_url = browser.url\n",
    "\n",
    "print(featured_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading the twitter page for martian weather and rigging up some beautiful soup\n",
    "mars_weather_url ='https://twitter.com/marswxreport?lang=en'\n",
    "weather_response = requests.get(mars_weather_url)\n",
    "w_soup = bs(weather_response.text, 'lxml')\n",
    "\n",
    "# looking for the correct div and class, then pulling the text from the first paragraph within that container/class\n",
    "mars_tweets = w_soup.find('div', class_ = 'stream').find_all(class_ = 'js-stream-item')\n",
    "for tweet in mars_tweets:\n",
    "    mars_text = tweet.find('div', class_ = 'js-tweet-text-container').p.text\n",
    "    if 'InSight sol' in mars_text:\n",
    "        mars_weather = mars_text.strip()\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the mars facts url and getting pandas to read the page\n",
    "mars_facts_url = 'https://space-facts.com/mars/'\n",
    "mars_facts_raw = pd.read_html(mars_facts_url)\n",
    "\n",
    "# moving the raw facts into a new variable\n",
    "mars_df = mars_facts_raw[0]\n",
    "\n",
    "# adding column names and setting the index\n",
    "mars_df.columns = ['Description', 'Value']\n",
    "mars_df.set_index('Description', inplace = True)\n",
    "\n",
    "# transforming the table to html and saving that to a variable\n",
    "mars_table_data = mars_df.to_html()\n",
    "print(mars_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the mars hemisphere urls and visiting the site\n",
    "mars_hemi_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "base_hemi_url = 'https://astrogeology.usgs.gov/'\n",
    "browser.visit(mars_hemi_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making some more beautifulsoup to get the hemisphere information\n",
    "hemi_response = requests.get(mars_hemi_url)\n",
    "h_soup = bs(hemi_response.text, 'lxml')\n",
    "\n",
    "# setting up the hemisphere_image_urls dictionary\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# getting a list of the image links (products)\n",
    "products = h_soup.find('div', class_ = 'result-list')\n",
    "hemispheres = products.find_all('div', class_ = 'item')\n",
    "\n",
    "# setting up the for loop to pull the information on all four image links\n",
    "for hemisphere in hemispheres:\n",
    "    \n",
    "    # getting the title for each item\n",
    "    title = hemisphere.find('h3').text\n",
    "    \n",
    "    # getting the partial link attached to each item and creating a full link\n",
    "    partial_link = hemisphere.find('a')['href']\n",
    "    image_link = base_hemi_url + partial_link\n",
    "    \n",
    "    # using splinter to go to the link and get the html to add to the beautifulsoup that's a-stewin'\n",
    "    browser.visit(image_link)\n",
    "    hemi_html = browser.html\n",
    "    h_soup_2 = bs(hemi_html, 'lxml')\n",
    "    \n",
    "    # using this new iteration of soup to find the link affiliated with downloads\n",
    "    downloads = h_soup_2.find('div', class_ = 'downloads')\n",
    "    image_url = downloads.find('a')['href']\n",
    "    \n",
    "    # loading the whole, hot mess into the dictionary generated earlier\n",
    "    hemisphere_image_urls.append({'title': title, 'img_url': image_url})\n",
    "\n",
    "print(hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
